{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np \n",
    "from einops import rearrange, reduce, repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EINOPS: Extremely useful libary for tensor multiplications\n",
    "Four main functions:\n",
    "- Rearrange: Instead of shuffling/transpose\n",
    "    - Adding dummy dimensions: Cleaner and easier to read than unsqueeze\n",
    "- Reduce\n",
    "- Repeat: Duplicates data across a new axis\n",
    "- Einsum: Einstein summation notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rearrange(ims, \"b h w c -> (b h) w c\") # e.g. Group batch and height of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = torch.tensor([1, 2, 3])\n",
    "repeated_einops = repeat(vector, 'n -> b n', b=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Einsum:\n",
    "Using $\\texttt{np.einsum()}$ for matrix multiplication: $M_{ij} = \\sum_{k} A_{ij}B_{kj} = A_{ik}B_{kj}$\n",
    "(Can handle more indices; same einsum functions for any dependency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.einsum('ik, kj -> ij', A, B) # e.g. Einsum method of matric multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers:\n",
    "- Input: tokens\n",
    "- Output: probability distribution over the next token (vector of length $\\texttt{vocab\\_size}$)\n",
    "- Good prediction $\\approx$ Good compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input: Tokenization\n",
    "- Input string is tokenized into sequence of numbers with a lookup table (vocab) + greedy longest match\n",
    "- Token: Token ID in the lookup table\n",
    "- Information only flows forward\n",
    "- Embedding matrix: context-aware high-dimensional representation of the tokenized input\n",
    "    - tokens -> token embedding\n",
    "    - token_ids -> position embedding\n",
    "    - token embedding + position embedding => transformer input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer block:\n",
    "- Parallel attention head $h_i$ run simultaneously and added to the information stream\n",
    "- MLP block: Single hidden layer, feed-forward neural network\n",
    "    - Linear layer (dimensional mapping) -> Activation fn. (e.g. GELU for GPT-2) -> Linear layer (reconstruct dimension)\n",
    "- LayerNorm block: Normalisation to ensure stability during training, ensures residual norm is $\\mathcal{N}(0, 1)$ (to avoid training loss explosion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Single-head) Attention mechanism:\n",
    "- Weight: fixed after training\n",
    "- Activation: intermediate value updated during training\n",
    "- Innovation: dot product K(keys) and Q(query) matrix to get a square matrix(K and V must be same shape, binary mapping), normalise $ * \\frac{1}{\\sqrt{d_{head}}}$(to ensure residual norm is unit variance, lower temperature)\n",
    "    - Causal mask: Discard top-right triangle, avoid model to attend to tokens in the future\n",
    "    - Maps to attention probabilities (probabilistic matrix) using Softmax\n",
    "    - <attention probabilities, V(value) matrix> (matmul by K dimension) => attention output\n",
    "\n",
    "(Multi-head) Attention mechanism: Parallel running for multiple single-head, attention output = sum(each head results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
